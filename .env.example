# LocalCowork Configuration
# Copy this file to .env and customize as needed
# All variables are prefixed with LOCALCOWORK_

# =============================================================================
# LLM Settings
# =============================================================================

# Ollama API endpoint
LOCALCOWORK_OLLAMA_URL=http://localhost:11434/api/generate

# Model to use (run 'ollama list' to see available models)
LOCALCOWORK_OLLAMA_MODEL=mistral

# Request timeout in seconds
LOCALCOWORK_OLLAMA_TIMEOUT=120

# Max tokens for LLM response
LOCALCOWORK_MAX_TOKENS=2048

# JSON parsing retry attempts
LOCALCOWORK_MAX_JSON_RETRIES=2

# =============================================================================
# Sandbox Settings (for isolated code execution)
# =============================================================================

# Execution timeout in seconds
LOCALCOWORK_SANDBOX_TIMEOUT=30

# Docker resource limits
LOCALCOWORK_SANDBOX_MEMORY_LIMIT=256m
LOCALCOWORK_SANDBOX_CPU_LIMIT=1
LOCALCOWORK_SANDBOX_PIDS_LIMIT=50
LOCALCOWORK_DOCKER_IMAGE=python:3.12-slim

# =============================================================================
# Server Settings (for web UI)
# =============================================================================

LOCALCOWORK_SERVER_HOST=127.0.0.1
LOCALCOWORK_SERVER_PORT=8000
LOCALCOWORK_SESSION_TIMEOUT=3600
LOCALCOWORK_MAX_HISTORY_MESSAGES=20

# =============================================================================
# Execution Settings
# =============================================================================

# Max code fix retries on error
LOCALCOWORK_MAX_CODE_RETRIES=2

# Enable parallel step execution
LOCALCOWORK_PARALLEL_EXECUTION=true

# Require approval before executing plans
LOCALCOWORK_REQUIRE_APPROVAL=true
