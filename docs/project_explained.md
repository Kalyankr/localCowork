# LocalCowork - Technical Deep Dive

This document provides a comprehensive explanation of the LocalCowork architecture, inspired by [Claude Cowork](https://support.claude.com/en/articles/13345190-getting-started-with-cowork).

---

## ğŸ“– Table of Contents

1. [Overview](#overview)
2. [Architecture](#architecture)
3. [Core Components](#core-components)
4. [Execution Flow](#execution-flow)
5. [Tool System](#tool-system)
6. [Sandbox Security](#sandbox-security)
7. [LLM Integration](#llm-integration)
8. [CLI Interface](#cli-interface)
9. [API Server](#api-server)

---

## Overview

LocalCowork is a **CLI-based agentic AI assistant** that transforms natural language requests into executable multi-step plans. Unlike traditional chatbots that respond to one prompt at a time, LocalCowork:

1. **Analyzes** your request
2. **Plans** a sequence of steps (as a DAG)
3. **Executes** steps with dependency resolution
4. **Reports** results with a friendly summary

### Key Differentiators from Claude Cowork

| Feature | Claude Cowork | LocalCowork |
|---------|---------------|-------------|
| LLM | Claude API (cloud) | Ollama (local) |
| Interface | Desktop GUI | CLI |
| Sandbox | VM-based | Docker container |
| Cost | Subscription | Free (local compute) |
| Privacy | Cloud processing | 100% local |

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER REQUEST                              â”‚
â”‚            "Organize my downloads by file type"                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CLI (cli.py)                             â”‚
â”‚  â€¢ Parse arguments (--yes, --dry-run, --verbose)                â”‚
â”‚  â€¢ Display plan with confirmation                                â”‚
â”‚  â€¢ Show live progress table                                      â”‚
â”‚  â€¢ Render final results                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PLANNER (planner.py)                        â”‚
â”‚  â€¢ Sends request + tool schema to LLM                           â”‚
â”‚  â€¢ Receives JSON plan with steps                                 â”‚
â”‚  â€¢ Validates and parses into Plan model                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     EXECUTOR (executor.py)                       â”‚
â”‚  â€¢ Builds dependency graph                                       â”‚
â”‚  â€¢ Runs steps in waves (parallel when possible)                 â”‚
â”‚  â€¢ Resolves variable interpolation                               â”‚
â”‚  â€¢ Stores outputs in context for dependent steps                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TOOL REGISTRY  â”‚ â”‚     SANDBOX      â”‚ â”‚   LLM CLIENT     â”‚
â”‚                  â”‚ â”‚                  â”‚ â”‚                  â”‚
â”‚  file_op         â”‚ â”‚  Docker-based    â”‚ â”‚  Ollama API      â”‚
â”‚  markdown_op     â”‚ â”‚  Python runner   â”‚ â”‚  JSON repair     â”‚
â”‚  pdf_op          â”‚ â”‚  Network: none   â”‚ â”‚  Error handling  â”‚
â”‚  data_op         â”‚ â”‚  Isolated fs     â”‚ â”‚                  â”‚
â”‚  text_op         â”‚ â”‚                  â”‚ â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Core Components

### 1. Models (`agent/orchestrator/models.py`)

Pydantic models define the data structures:

```python
class Step:
    id: str              # Unique identifier (e.g., "list_all")
    description: str     # Human-readable description
    action: str          # Tool to use ("file_op", "python", etc.)
    args: Dict[str, Any] # Arguments for the tool
    depends_on: List[str] # Step IDs that must complete first

class Plan:
    steps: List[Step]    # Ordered list of steps

class StepResult:
    step_id: str
    status: str          # "success", "error", "skipped"
    output: Any          # Result data
    error: str           # Error message if failed
```

### 2. Planner (`agent/orchestrator/planner.py`)

The planner converts natural language to a structured plan:

```python
def generate_plan(user_request: str) -> Plan:
    prompt = PLANNER_PROMPT + user_request
    data = call_llm_json(prompt)  # LLM returns JSON
    return Plan(**data)
```

The `PLANNER_PROMPT` contains:
- Tool schema (available operations)
- Examples of good plans
- Rules for JSON formatting
- Dependency guidelines

### 3. Executor (`agent/orchestrator/executor.py`)

The executor runs the plan with parallel execution:

```python
class Executor:
    async def run(self) -> dict:
        # Wave-based execution
        while not all_complete:
            ready_steps = get_steps_with_deps_met()
            
            if parallel and len(ready_steps) > 1:
                # Run independent steps concurrently
                await asyncio.gather(*[run_step(s) for s in ready_steps])
            else:
                # Sequential execution
                for step in ready_steps:
                    await run_step(step)
```

Key features:
- **Dependency resolution**: Steps wait for their dependencies
- **Context propagation**: Output from step A is available to step B
- **Variable interpolation**: `"path": "list_all"` resolves to actual file list
- **Progress callbacks**: CLI receives real-time status updates

### 4. Tool Registry (`agent/orchestrator/tool_registry.py`)

A simple registry pattern for tool dispatch:

```python
class ToolRegistry:
    def register(self, name: str, func: Callable)
    def get(self, name: str) -> Callable
    def list_tools(self) -> List[str]
    def has(self, name: str) -> bool
```

---

## Execution Flow

### Example: "Organize downloads into Images and PDFs"

```
User Request
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM generates plan:                 â”‚
â”‚                                     â”‚
â”‚ Step 1: list_all (file_op:list)    â”‚
â”‚    â””â”€â–º Step 2: categorize (python) â”‚
â”‚           â”œâ”€â–º Step 3: move_imgs    â”‚
â”‚           â””â”€â–º Step 4: move_pdfs    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Executor runs:                      â”‚
â”‚                                     â”‚
â”‚ Wave 1: [list_all]                 â”‚
â”‚    Output: [{path, name, ...}, ...]â”‚
â”‚                                     â”‚
â”‚ Wave 2: [categorize]               â”‚
â”‚    Code: imgs = [f for f in ...]   â”‚
â”‚    Output: {imgs: [...], pdfs: [...]}â”‚
â”‚                                     â”‚
â”‚ Wave 3: [move_imgs, move_pdfs]     â”‚  â† Parallel!
â”‚    Both run simultaneously          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
Results + Summary
```

### Variable Resolution

The executor supports several interpolation patterns:

```python
# Direct reference
"path": "list_all"  â†’  [{...}, {...}]

# Dictionary access
"src": "categorize['imgs']"  â†’  [img files]

# Path concatenation
"dest": "base_dir/Images"  â†’  "/home/user/Downloads/Images"
```

---

## Tool System

### Available Tools

| Tool | Operations | Description |
|------|------------|-------------|
| `file_op` | list, move, mkdir, read, write, rename | File system operations |
| `markdown_op` | create | Generate markdown files |
| `pdf_op` | extract | Extract PDF metadata |
| `data_op` | csv_to_excel | Data format conversion |
| `text_op` | summarize, extract, transform | LLM-powered text processing |
| `python` | (code execution) | Run Python in sandbox |

### Adding a New Tool

1. Create `agent/tools/my_tool.py`:
```python
def my_operation(arg1: str, arg2: int) -> str:
    # Implementation
    return result

def dispatch(op: str, **kwargs):
    if op == "my_op":
        return my_operation(kwargs["arg1"], kwargs["arg2"])
    raise ValueError(f"Unknown op: {op}")
```

2. Register in `agent/tools/__init__.py`:
```python
def create_default_registry():
    registry.register("my_op", my_tool.dispatch)
```

3. Add to `PLANNER_PROMPT` in `agent/llm/prompts.py`:
```python
- "my_op"
    - args: {"op": "my_op", "arg1": str, "arg2": int}
```

---

## Sandbox Security

### Docker Isolation

Python code from plans runs in a secure Docker container:

```python
cmd = [
    "docker", "run", "--rm",
    "--network", "none",      # No internet access
    "-v", f"{tmpdir}:/app",   # Only mount temp directory
    "python:3.12-slim",
    "python", "script.py"
]
```

Security measures:
- **No network**: `--network none` prevents data exfiltration
- **Isolated filesystem**: Only temp directory is mounted
- **Timeout**: Execution limited to 30 seconds
- **Read-only context**: Variables injected, not actual file access

### Context Injection

The sandbox receives variables from previous steps:

```python
# Injected at top of script
list_all = [{"path": "/home/user/file.txt", ...}]
categorize = None

# User's code
imgs = [f for f in list_all if f['name'].endswith('.jpg')]

# Capture results (appended automatically)
print(f'__RESULT__:' + json.dumps(categorize))
print(f'__TRACE_VARS__:' + json.dumps(locals()))
```

---

## LLM Integration

### Ollama Configuration

```python
# Environment variables
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434/api/generate")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "mistral")
OLLAMA_TIMEOUT = int(os.getenv("OLLAMA_TIMEOUT", "120"))
```

### JSON Repair

Local LLMs sometimes produce malformed JSON. The `repair_json()` function handles:
- Trailing commas
- Unquoted values
- Literal newlines in strings
- Missing brackets

```python
def call_llm_json(prompt: str) -> dict:
    raw = call_llm(prompt)
    try:
        return json.loads(raw)
    except json.JSONDecodeError:
        return repair_json(raw)  # Attempt to fix
```

---

## CLI Interface

### Commands

```bash
# Run a task
localcowork run "organize my downloads" [options]

# Start API server
localcowork serve --host 127.0.0.1 --port 8000
```

### Options

| Flag | Short | Description |
|------|-------|-------------|
| `--yes` | `-y` | Skip confirmation |
| `--dry-run` | `-n` | Show plan only |
| `--no-parallel` | `-s` | Sequential execution |
| `--verbose` | `-v` | Debug logging |

### Progress Display

```
 Step                 Status       Description
 list_all             âœ“ done       file_op
 categorize           â–¶ running    Filter files into categories
 move_imgs            â³ pending   file_op
 move_pdfs            â³ pending   file_op
```

---

## API Server

FastAPI server for programmatic access:

```python
# POST /tasks
{
    "request": "organize my downloads"
}

# Response
{
    "task_id": "uuid",
    "plan": {...},
    "results": {...}
}

# GET /health
{"status": "ok"}
```

### Query Parameters

- `parallel=true|false` - Enable/disable parallel execution

---

## Directory Structure

```
localCowork/
â”œâ”€â”€ main.py                 # Entry point for uvicorn
â”œâ”€â”€ pyproject.toml          # Project dependencies
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py              # Typer CLI application
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ client.py       # Ollama API client
â”‚   â”‚   â””â”€â”€ prompts.py      # System prompts for planner
â”‚   â”œâ”€â”€ orchestrator/
â”‚   â”‚   â”œâ”€â”€ executor.py     # Plan execution engine
â”‚   â”‚   â”œâ”€â”€ models.py       # Pydantic data models
â”‚   â”‚   â”œâ”€â”€ planner.py      # LLM-based plan generation
â”‚   â”‚   â”œâ”€â”€ server.py       # FastAPI server
â”‚   â”‚   â””â”€â”€ tool_registry.py # Tool registration
â”‚   â”œâ”€â”€ sandbox/
â”‚   â”‚   â””â”€â”€ sandbox_runner.py # Docker-based code execution
â”‚   â””â”€â”€ tools/
â”‚       â”œâ”€â”€ file_tools.py   # File operations
â”‚       â”œâ”€â”€ markdown_tools.py
â”‚       â”œâ”€â”€ pdf_tools.py
â”‚       â”œâ”€â”€ data_tools.py
â”‚       â””â”€â”€ text_tools.py
â””â”€â”€ docs/
    â”œâ”€â”€ progress.md         # Enhancement tracking
    â””â”€â”€ project_explained.md # This file
```

---

## Dependencies

| Package | Purpose |
|---------|---------|
| `typer` | CLI framework |
| `rich` | Terminal UI (tables, progress, panels) |
| `pydantic` | Data validation |
| `requests` | HTTP client for Ollama |
| `fastapi` | API server |
| `uvicorn` | ASGI server |
| `pandas` | Data operations |
| `pypdf` | PDF processing |

---

## Running Locally

### Prerequisites

1. **Python 3.12+**
2. **Docker** (for sandbox)
3. **Ollama** with a model installed

### Setup

```bash
# Install Ollama and pull a model
curl -fsSL https://ollama.com/install.sh | sh
ollama pull mistral

# Clone and install
git clone https://github.com/yourusername/localCowork.git
cd localCowork
uv pip install -e .

# Run
localcowork run "list files in my downloads"
```

---

## Design Principles

1. **Privacy First**: All processing happens locally
2. **Transparency**: Users see the plan before execution
3. **Safety**: Sandboxed execution for untrusted code
4. **Extensibility**: Easy to add new tools
5. **Reliability**: Error handling and JSON repair for local LLMs
