# ğŸ¤– LocalCowork

**A privacy-first AI agent that runs entirely on your machine - inspired by Claude's Cowork.**

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![Ollama](https://img.shields.io/badge/LLM-Ollama-orange.svg)](https://ollama.com/)

---

LocalCowork is a **pure agentic** AI assistant that uses shell commands, Python, and web search to accomplish tasks-adapting step-by-step based on what it discovers. **100% local, 100% private.**

```
  â–ˆâ–ˆâ•—  LocalCowork v0.3.0  â— mistral
  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  Type a request or /help
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
  â”‚ > organize my downloads by file type â”‚
  â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

  â— $ mkdir -p ~/Downloads/{Images,PDFs,Documents}  â—â—â—

  â—† LocalCowork
    Done! Organized your downloads into 3 folders:
    - Images/ (12 files)
    - Documents/ (5 files)
    - PDFs/ (8 files)
```

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ”’ **Privacy First** | All processing happens locally-your files never leave your machine |
| ğŸ¤– **Pure Agentic** | ReAct loop: Observe â†’ Think â†’ Act â†’ Repeat |
| ğŸš **Shell + Python** | Uses tools you already knowâ€”no complex APIs |
| ğŸŒ **Web Search** | Search the web and fetch webpage content |
| ğŸ’¬ **Conversational** | Remembers context across your session |
| ğŸ³ **Sandboxed Python** | Code runs in isolated Docker containers |
| â±ï¸ **Long-running Tasks** | 10 min shell / 5 min Python timeouts |
| ğŸ¨ **Professional CLI** | Clean interface with progress indicators |

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.12+**
- **Docker** (optional, for sandboxed Python execution)
- **Ollama** with a model installed

### Installation

```bash
# 1. Install Ollama and pull a model
curl -fsSL https://ollama.com/install.sh | sh
ollama pull mistral

# 2. Clone and install
git clone https://github.com/Kalyankr/localCowork.git
cd localCowork
uv sync  # or: pip install -e .

# 3. Start the agent
localcowork
```

---

## ğŸ“– Usage

### Interactive Mode

```bash
localcowork                    # Start interactive mode
localcowork --model llama3     # Use a different model
localcowork serve              # Start the web API server
```

### Commands

| Command | Description |
|---------|-------------|
| `/help` | Show help and examples |
| `/clear` | Clear screen and history |
| `/status` | Show current settings |
| `/history` | Show conversation history |
| `/model <name>` | Switch to a different model |
| `/quit` | Exit the agent |

### Example Conversations

```
You: analyze sales.csv and show top customers

  â— Running Python...  â—

  â—† LocalCowork
    ğŸ“Š Sales Analysis (1,247 records):
    - Total revenue: $2.4M
    - Top customer: Acme Corp ($89,400)
    - Growth: +38% YoY
```

```
You: search for python asyncio best practices

  â— Searching the web...  â—

  â—† LocalCowork
    Here are some asyncio best practices:
    1. Use async/await consistently - https://...
    2. Avoid blocking calls in async code - https://...
    3. Use asyncio.gather for concurrent tasks

You: what's the weather API endpoint for Seattle?

  â— $ curl wttr.in/Seattle?format=3  â—

  â—† LocalCowork
    Seattle: â›… +12Â°C
```

---

## ğŸ§  How It Works

LocalCowork uses a **ReAct (Reasoning + Acting)** loop:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Request   â”‚â”€â”€â”€â”€â–¶â”‚   Think     â”‚â”€â”€â”€â”€â–¶â”‚    Act      â”‚
â”‚             â”‚     â”‚  (Ollama)   â”‚     â”‚ (tools)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–²                    â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              Observe result
```

### Available Tools

| Tool | What It Does |
|------|--------------|
| **shell** | Run any bash command (`ls`, `mv`, `grep`, `curl`, etc.) |
| **python** | Execute Python code (pandas, requests, openpyxl available) |
| **web_search** | Search the web via DuckDuckGo |
| **fetch_webpage** | Fetch and extract text from URLs |

### Example Agent Reasoning

```
Request: "Search for Python 3.13 new features and summarize"

Step 1:
  Thought: I'll search the web for Python 3.13 features
  Action: web_search â†’ {"query": "Python 3.13 new features"}

Step 2:
  Thought: Found good results. Let me fetch the official docs.
  Action: fetch_webpage â†’ {"url": "https://docs.python.org/3.13/whatsnew"}

Step 3:
  Thought: Got the content. I can now summarize for the user.
  Action: complete â†’ "Python 3.13 brings: 1. Improved error messages..."
```

---

## âš™ï¸ Configuration

Environment variables (prefix: `LOCALCOWORK_`):

| Variable | Default | Description |
|----------|---------|-------------|
| `LOCALCOWORK_OLLAMA_MODEL` | `mistral` | LLM model to use |
| `LOCALCOWORK_OLLAMA_URL` | `http://localhost:11434` | Ollama endpoint |
| `LOCALCOWORK_SHELL_TIMEOUT` | `600` | Shell command timeout (seconds) |
| `LOCALCOWORK_SANDBOX_TIMEOUT` | `300` | Python execution timeout (seconds) |
| `LOCALCOWORK_MAX_AGENT_ITERATIONS` | `15` | Max ReAct loop iterations |

```bash
# Use a different model with longer timeout
LOCALCOWORK_OLLAMA_MODEL=llama3 LOCALCOWORK_SHELL_TIMEOUT=900 localcowork
```

---

## ğŸ”’ Security

### Safety Confirmations
Dangerous operations (file deletion, system changes) require explicit user confirmation before execution.

### Path Protection
- **Path Traversal Protection**: Blocks `../../etc/passwd` style attacks
- **Sensitive Path Blocking**: Denies access to `/etc/shadow`, `~/.ssh`, etc.

### Sandboxed Python (Docker mode)
- No network access (`--network none`)
- Non-root user (`--user 1000:1000`)
- All capabilities dropped (`--cap-drop ALL`)
- Read-only filesystem (`--read-only`)
- Limited resources (256MB RAM, 1 CPU, 50 PIDs)

---

## ğŸ“ Project Structure

```
localCowork/
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ cli/
â”‚   â”‚   â”œâ”€â”€ __init__.py        # CLI entry point (Typer)
â”‚   â”‚   â”œâ”€â”€ agent_loop.py      # Interactive agent loop
â”‚   â”‚   â””â”€â”€ console.py         # Rich console utilities
â”‚   â”œâ”€â”€ config.py              # Centralized settings (Pydantic)
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ client.py          # Ollama client
â”‚   â”‚   â””â”€â”€ prompts.py         # ReAct prompts
â”‚   â”œâ”€â”€ orchestrator/
â”‚   â”‚   â”œâ”€â”€ react_agent.py     # The ReAct agent (core)
â”‚   â”‚   â”œâ”€â”€ agent_models.py    # Agent state models
â”‚   â”‚   â”œâ”€â”€ server.py          # FastAPI server
â”‚   â”‚   â””â”€â”€ deps.py            # Dependency injection
â”‚   â”œâ”€â”€ sandbox/
â”‚   â”‚   â””â”€â”€ sandbox_runner.py  # Docker/permissive sandbox
â”‚   â”œâ”€â”€ web.py                 # Web search & fetch tools
â”‚   â”œâ”€â”€ safety.py              # Command/code analysis
â”‚   â””â”€â”€ security.py            # Path validation
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

## ğŸ¤ Contributing

Contributions welcome!

```bash
# Install dev dependencies
uv sync --all-extras

# Run tests
uv run pytest

# Format & lint
uv run ruff format .
uv run ruff check --fix .
```

---

<p align="center">
  <b>Built for local-first AI â€” Inspired by Claude's Cowork</b>
</p>
